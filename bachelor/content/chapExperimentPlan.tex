\chapter{Versuche}

\section{Versuchsplan}

Um systematisch zu bewerten, ob RAG-Bewertungstools für den Einsatz in kleineren Unternehmen bereit sind, sind umfassende Versuche erforderlich. Der folgende versuchsplan skizziert die wichtigsten Variablen, die Methodik und die Bewertungskriterien.

\subsection{Forschungsfragen}

Der Versuch wird die folgenden zentralen Forschungsfragen behandeln:

\begin{enumerate}
    \item Sind aktuelle RAG-Bewertungsframeworks in Bezug auf Kosten, Komplexität und Ressourcenanforderungen für den Einsatz in kleinen Unternehmen geeignet?
    \item Wie beeinflussen verschiedene Dokumenttypen und Datenvolumina die Qualität von Abruf und Generierung?
    \item Wie zuverlässig und konsistent sind die verfügbaren Bewertungsmetriken zur Beurteilung der RAG-Leistung?
    \item Was ist das optimale Gleichgewicht zwischen Kosten, Leistung und Implementierungskomplexität für jeden Anwendungsfall in kleinen Unternehmen?
\end{enumerate}

\subsection{Variablen in den Versuchen}

\subsubsection{Dokumenttypen}
Verschiedene Dokumentformate werden getestet, um die Vielseitigkeit des Systems zu bewerten:
\begin{itemize}
    \item PDF (.pdf)
    \item Klartext (.txt)
    \item Word-Dokumente (.docx, .doc)
    \item Excel-Tabellen (.xlsx, .xls)
    \item CSV-Dateien (.csv)
    \item E-Mails (.eml)
    \item PowerPoint-Präsentationen (.pptx, .ppt)
\end{itemize}

\subsubsection{Datenvolumen}
Die Skalierbarkeit des Systems wird wie bereits beschrieben mit unterschiedlichen Datenmengen getestet, 10, 100 und 400 Dokumente.
\begin{itemize}
    \item Für die Versuche mit \textbf{10 Dokumenten} werden existierende Dokumente ausgewählt.
    \item Für die Versuche mit \textbf{100} müssen zusätzliche Dokumente generiert werden, vorzugsweise mit einem LLM.
    \item Für die Versuche mit \textbf{400 Dokumenten} wird zusätzlich Code verwendet.
\end{itemize}

\subsubsection{Modelle zur Bewertung}
Mehrere Modelle werden bewertet, die verschiedene Kostenschichten und Fähigkeiten repräsentieren. Hierbei ist es wichtig zu überlegen, welche Optionen für kleine Unternehmen gültige Anwendungsfälle sind.\\
\textbf{Open-Source-Modelle} (z.B. Llama 2, Mistral 7B, Deepseek R1) bieten eine Vielzahl von Vorteilen, wie die Möglichkeit, sie zu modifizieren und mehr Kontrolle über die Daten zu haben, was rechtliche Vorteile bietet, aber auch Nachteile. Entscheident ist zudem wieder die Technische Kompeten welche benötigt wurd um diese Modelle selber zu Hosten.\\
\textbf{Mittelklasse-API-Modelle} (z.B. Claude Haiku, GPT-3.5 Turbo) sind günstiger als die Hochleistungsmodelle und bieten dennoch eine gute Leistung. Da sie nicht Open Source sind, bieten sie weniger Kontrolle über die Daten und das Modell selbst. Manchmal muss man mehr für private Instanzen zahlen.\\
\textbf{Hochleistungsmodelle} (z.B. GPT-4, Claude 3 Opus) sind die teuerste Option, bieten aber auch die beste Leistung, sowohl in Bezug auf Geschwindigkeit als auch auf die Qualität der generierten Antworten. Sie haben ähnliche Vor- und Nachteile wie die Mittelklasse-API-Modelle.\\
% TODO: artikel 53 und Anhang 12 und 13
% TODO: erklären

\subsubsection{Bewertungsmetriken}
Während des Versuchs werden neben der menschlichen Bewertung zwei Frameworks zur Bewertung verwendet.
Giskard und RAGAS werden die später beschriebenen Metriken generieren, die später verglichen und bewertet werden können.
Die menschliche Bewertung wird als subjektives Maß verwendet, um die Ergebnisse der anderen beiden zu vergleichen.

\subsection{Kosten- und Zeitanalyse}
Ob wir dies tun wollen, ist noch nicht klar. RAGAS bietet Kostenberechnung an, aber ich habe es mir noch nicht angesehen.

\subsection{Versuchsprotokoll}

\begin{enumerate}
    \item \textbf{Dokumentensammlung und -vorbereitung}
    Die Dokumente werden in allen oben genannten Zielformaten gesammelt.
    
    \item \textbf{Testset-Generierung}
    Verschiedene Fragetypen (faktisch, inferentiell, vergleichend) werden generiert und Referenzantworten zur Bewertung erstellt.
    Dies geschieht automatisch durch das RAGAS-Framework.
    Das Testset wird manuell auf Qualität und Abdeckung validiert, wobei dies anhand einer Reihe zufälliger Proben erfolgt.
    
    \item \textbf{Systemkonfiguration}
    Die Einbettungsmodelle und Parameter werden konfiguriert, Vektorspeicher mit konsistenten Einstellungen eingerichtet und die Bewertungsframeworks implementiert.
    
    \item \textbf{Durchführung der Bewertung}
    Die hochgeladenen Dateien, generierten Dokumente und das Testset werden wiederverwendet und zunächst erstellt.
    Anschließend wird die Bewertungspipeline ausgeführt und die Ergebnisse werden aufgezeichnet.
    
    \item \textbf{Analyse und Berichterstattung}
    Eine vergleichende Analyse über alle Variablen hinweg wird durchgeführt, einschließlich einer Kosten-Nutzen-Analyse für die geschäftliche Entscheidungsfindung und Empfehlungen für optimale Konfigurationen.
\end{enumerate}

\subsection{Bewertungskriterien für die Geschäftstauglichkeit}

Die endgültige Bewertung wird RAG-Systeme in diesen Dimensionen bewerten:
\begin{itemize}
    \item \textbf{Implementierungskomplexität}: Wie schwierig ist die Einrichtung und Wartung?
    \item \textbf{Kostenvorhersehbarkeit}: Sind die Kosten stabil und vorhersehbar?
    \item \textbf{Leistungszuverlässigkeit}: Sind die Ergebnisse konsisten und nicht komplett anders bei jeder Bewertung.
    \item \textbf{Skalierbarkeit}: Wie gut bewältigt das System wachsende Datenanforderungen?
    %\item \textbf{Integrationspotenzial}: Kann es mit bestehenden Geschäftssystemen verbunden werden?
    %\item \textbf{Gesamtkosten des Eigentums}: Was sind die vollständigen Kosten über die Zeit?
\end{itemize}

Dieser Ansatz mit Versuchen bietet einen umfassenden Rahmen, um zu bewerten, ob aktuelle RAG-Bewertungstools ausreichend ausgereift für die Einführung in kleinen Unternehmen sind, mit klaren Anleitungen zu optimalen Konfigurationen und Implementierungsstrategien. 