\chapter*{Kurzfassung/\emph{Abstract}}
\label{chap:abstract}

Heutzutage gewinnen LLMs wie ChatGPT und Gemini immer weiter an beliebtheit. In Kombination mit relaevanten, häufig firmeninternen Dokumenten, können Sie noch hilfreicher sein.
Wenn man mithilfe von LLMs jetzt relevante Daten in einer Datenbank sucht um damit Kontextabhängige Fragen zu beantworten hat man ein RAG.
Wie gut RAGs jedoch funktioniert hängt von vielen unterscheidlichen Faktoren, diese System manuell zu bewerten erfordert einen Zeit und Kostenintensiven Prozess.\\
Retrieval Augmented Generation Assessment (RAGAS) hat ein System entwickelt um RAGs automatisch zu bewerten.
Wie gut diese automatisierte Bewertung von RAGs mithilfe von RAGAS funktioniert ist der Hauptfokus dieser Arbeit. Dabei werden besonders drei Bereiche Untersucht, die generierten Fragebögen, die Bewertung und die Zuverlässigkeit bei mehreren Wiederholungen.

Es war klar zu beobachten, dass das genutzte LLM eine große Rolle spielte, insbesondere bei der Fragebogen generierung als auch bei der Bewertung.
Bessere LLMs haben sowohl weniger Fehler während der generierung der Fragebögen gemacht als auch bessere Fragen generiert.
Es hat sich außerdem gezeigt, dass Fehler aus den Fragebögen sich durch die Bewertung ziehen und dadurch die Bewertung negativ beeinflussen.
Die Metriken und Bewertungen waren konstant über mehrere Bewertungen hinweg und es waren nur minimale Schwankungen fest zu stellen.

Zusammenfassen lässt sich sagen, dass Ragas nicht als alleiniger Faktor eingesetzt werden kann und mindestens bei der Testset generierung eine menschliche Überrüfung stattfinden sollte.
Außerdem wäre ein LLM welches gegen Entgelt schnell Ergebnisse liefert zu empfehlen.
Zukünftig lässt sich Ragas mit weiterentwickelten LLMs und einer verbesserten Fragebogen generierung vielleicht komplett automatisieren
!TODO!




Schlagwörter/Schlüsselwörter: gegebenenfalls Angabe von 3 bis 10 Schlagwörtern.
LLM, KI, RAG, Ragas, Automatisierung