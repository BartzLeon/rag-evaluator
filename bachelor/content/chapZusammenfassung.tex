\chapter{Zusammenfassungen}

\section{Benutzung von RAGAS}
Dank der vielen Integrationen hat sich die Verwendung von RAGAS als einfach herausgestellt.

\section{Fragebögen}
Die Generierung von Fragebögen ist eines der Alleinstellungsmerkmale von RAGAS. Die Generierung der Fragebögen hat sich aus softwaretechnischer Sicht als unkompliziert erwiesen.
Es gab zu den wichtigen Themen ausreichend Dokumentation und Beispiele.

Es ist mit RAGAS möglich, Fragebögen zu generieren, jedoch gibt es mehrere Faktoren, die Qualität und Praxistauglichkeit beeinflussen:
\begin{itemize}
    \item Die Fähigkeit des LLMs, zuverlässig hochwertige Antworten zu generieren.
    \item Die Dokumente: Je komplexer und zusammenhangsloser die Dokumente sind, desto schlechter lassen sich Fragen generieren.
\end{itemize}

Bei der Generierung von Fragebögen mit DeepSeek kam es alleine durch die nicht generierten oder zu irrelevanten Themen generierten Fragen zu einer Fehlerquote von bis zu 45 \%.
Selbst bei händisch ausgewählten Dokumenten lag die Fehlerquote bei mindestens 16 \%.

Die händische Überprüfung hat dann weiter gezeigt, dass DeepSeek Probleme mit der konstanten Generierung von sinnvollen Fragen hat.
Hier wiesen bis zu 65 \% der Fragen für ungefilterte Dokumente Mängel auf! Selbst bei den gefilterten Dokumenten waren mindestens 30 \% mangelbehaftet.

Die Generierung von Fragebögen mit OpenAIs GPT-4 hatte in Bezug auf nicht generierte oder Fragen zu irrelevanten Themen eine deutlich niedrigere Fehlerquote.
Es gibt einen Ausreißer mit 20 \% , der Rest bleibt jedoch deutlich unter 10 \%.
Die manuelle Auswertung hat hier aber auch gezeigt, dass viele Fragen, bis zu 60 \% bei ungefilterten Dokumenten, Mängel aufweisen.
Bei gefilterten Dokumenten kommt GPT-4 auf durchschnittlich 17,5 \% und halbiert damit die Fehlerquote im Vergleich mit DeepSeek.

Die Qualität des Fragebögen ist entscheidend, da sich hier entstandene Fehler weiter bis in die Bewertung durchziehen und eine korrekte Bewertung des eigentlichen RAGs verzerren!

Die bei den Versuchen generierten Fragebögen lassen Zweifel an einer zuverlässigen und hochwertigen Generierung von Fragen aufkommen.

\section{Bewertung}

Auch das Generieren von Bewertungen hat sich mithilfe von RAGAS als einfach umzusetzen erwiesen.
Sowohl das Tracing als auch die Kostenberechnung waren für die unterstützten Modelle problemlos zu benutzen.
Das Tracing erlaubt außerdem einen tieferen Blick in die Berechnung der Metriken und macht das ganze System transparenter.

Es hat sich jedoch bei der manuellen Durchsicht gezeigt, dass hier bei DeepSeek 60 \% und bei GPT-4 30 \% der Fragen nicht richtig beantwortet wurden.
Dies lässt sich teils auf die ungültigen Fragen in den Fragebögen zurückführen.

Bei den Metriken lässt sich sagen, dass die Metriken zum Kontext (\texttt{recall} und \texttt{precision}) gut abschneiden.
Die \texttt{faithfulness} zeigt eine erhöhte Abweichung zu der menschlichen Einschätzung und sollte mit einer Toleranz von 10 \% beachtet werden.

Die \texttt{Answer Relevancy} hat die größte Abweichung; hier fällt auf, dass sowohl höhere als auch niedrigere Werte erwartet wurden.

\section{Zuverlässigkeit}

Sowohl die mehrfache gesamte Bewertung also auch das mehrfache Ausführen einzelner Metriken haben gezeigt, dass die Bewertung einer Schwankung von wenigen Prozentpunkten unterliegt.
Es war zu sehen, dass LLMs mit mehr Parametern geringere Schwankungen gab.
Hier war der Unterschied zwischen DeepSeek und OpenAI jedoch deutlich geringer, als bei der Fragebogengenerierung oder der Bewertung.
Insgesammt lässt sich sagen, dass die minimalen Schwankungen die Praxistauglichkeit nicht beeinflussen und sich auf die Ergebnisse verlassen werden kann.

\section{Fazit}
Insgesamt ist RAGAS kein kompletter Ersatz für die menschliche Bewertung von RAGs. Die Idee hinter RAGAS, Fragen ohne menschliches Zutun zu generieren, um Zeit zu sparen, ist mit besseren LLMs teilweise gelungen.
Um jedoch ein aussagekräftiges und zuverlässiges Ergebnis zu generieren, ist eine menschliche Kontrolle an mehreren Stellen notwendig.
Zuerst bei der Auswahl der Dokumente: Hier muss sowohl ein Verständnis vorhanden sein, wie gut LLMs mit welchen Daten umgehen können, als auch welche Daten für das KMU relevant sind.
Nach der Generierung der Fragebögen sollte erneut ein Mensch die Fragen überprüfen, um grob falsche Fragen zumindest zu löschen.

Da die Berichte relativ konstante Bewertungen abgeben, lassen sich dann durchaus Verschlechterungen oder Verbesserungen am RAG messen.
Die Metriken geben Aufschluss darüber, welcher Teil des Systems nicht funktioniert; diese Zusammenhänge ließen sich sehr gut sehen.

Insgesamt muss jedoch auch der Zeit- und Kostenaufwand für eine solche Bewertung in Betracht gezogen werden.
Für eine aktive Entwicklung ist das Abwarten von 17 Stunden für eine Bewertung eines RAGs nicht praxistauglich und ein Hindernis.
Eine Bewertung innerhalb von einer Stunde ist praxistauglich, ist jedoch ein Kostenfaktor. Hier muss genauer der Anwendungsfall betrachtet werden.

\section{Zukunftsausblick}

Für Unternehmen bieten LLMs und RAGs großes Potenzial für Kosteneinsparungen; die Qualitätskontrolle spielt dabei eine immer größere Rolle.
RAGAS bietet gute Ansätze, um die Qualitätskontrolle zu automatisieren. Dass RAGAS in Zukunft in die Prozesse zur Bewertung solcher Systeme einfließt, ist daher sehr wahrscheinlich.

\section{Reflexion der Arbeit}