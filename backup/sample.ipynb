{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T16:20:45.276923Z",
     "start_time": "2025-01-03T16:20:38.344929Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install huggingface_hub datasets pandas tqdm -q",
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\r\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\r\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T16:47:50.092049Z",
     "start_time": "2025-01-03T16:47:48.757109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import InferenceClient, notebook_login\n",
    "\n",
    "tqdm.pandas()  # load tqdm's pandas support\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "notebook_login()"
   ],
   "id": "26542cb34a4510f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54d61838cf694523904ff6999fcdf735"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T16:48:09.646897Z",
     "start_time": "2025-01-03T16:48:09.262028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "# Test your LLM client\n",
    "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
   ],
   "id": "f8e160939104f625",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nI’m good, thanks. I’m just about to go to the gym.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T17:12:43.447359Z",
     "start_time": "2025-01-03T17:11:43.984871Z"
    }
   },
   "cell_type": "code",
   "source": "ratings = load_dataset(\"McGill-NLP/feedbackQA\")[\"train\"]",
   "id": "a2f790937d63da4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.43k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20644735d0d64de8a0dc7e1105c041f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leonbartz/.cache/huggingface/datasets/downloads/3fd9d8b80db4e8ce3de049be3beae23e75ccf024624d7daf007ced34c6f446d4/feedback_test.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/5660 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9b3f920c5b74e12921ff438e92b2cde"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotADirectoryError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1607\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split_single\u001B[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001B[0m\n\u001B[1;32m   1606\u001B[0m _time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m-> 1607\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, record \u001B[38;5;129;01min\u001B[39;00m generator:\n\u001B[1;32m   1608\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_shard_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m writer\u001B[38;5;241m.\u001B[39m_num_bytes \u001B[38;5;241m>\u001B[39m max_shard_size:\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/McGill-NLP--feedbackQA/20c8f938f417c88303bb7041cea9554c1d14667686d7d7c5dda83dd4f39e5dc4/feedbackQA.py:109\u001B[0m, in \u001B[0;36mFeedbackQA._generate_examples\u001B[0;34m(self, filepath)\u001B[0m\n\u001B[1;32m    108\u001B[0m key \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 109\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filepath, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    110\u001B[0m     fbqa \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/streaming.py:75\u001B[0m, in \u001B[0;36mextend_module_for_streaming.<locals>.wrap_auth.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(function)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, download_config\u001B[38;5;241m=\u001B[39mdownload_config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/utils/file_utils.py:943\u001B[0m, in \u001B[0;36mxopen\u001B[0;34m(file, mode, download_config, *args, **kwargs)\u001B[0m\n\u001B[1;32m    942\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblock_size\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 943\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(main_hop, mode, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    944\u001B[0m \u001B[38;5;66;03m# add headers and cookies for authentication on the HF Hub and for Google Drive\u001B[39;00m\n",
      "\u001B[0;31mNotADirectoryError\u001B[0m: [Errno 20] Not a directory: '/Users/leonbartz/.cache/huggingface/datasets/downloads/3fd9d8b80db4e8ce3de049be3beae23e75ccf024624d7daf007ced34c6f446d4/feedback_train.json'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mDatasetGenerationError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m ratings \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMcGill-NLP/feedbackQA\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/load.py:2151\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[1;32m   2148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\u001B[38;5;241m.\u001B[39mas_streaming_dataset(split\u001B[38;5;241m=\u001B[39msplit)\n\u001B[1;32m   2150\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[0;32m-> 2151\u001B[0m builder_instance\u001B[38;5;241m.\u001B[39mdownload_and_prepare(\n\u001B[1;32m   2152\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[1;32m   2153\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[1;32m   2154\u001B[0m     verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[1;32m   2155\u001B[0m     num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[1;32m   2156\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m   2157\u001B[0m )\n\u001B[1;32m   2159\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[1;32m   2160\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2161\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[1;32m   2162\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:924\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m    922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    923\u001B[0m     prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[0;32m--> 924\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[1;32m    925\u001B[0m     dl_manager\u001B[38;5;241m=\u001B[39mdl_manager,\n\u001B[1;32m    926\u001B[0m     verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[1;32m    927\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs,\n\u001B[1;32m    928\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs,\n\u001B[1;32m    929\u001B[0m )\n\u001B[1;32m    930\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[1;32m    931\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1648\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001B[0m\n\u001B[1;32m   1647\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_download_and_prepare\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager, verification_mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs):\n\u001B[0;32m-> 1648\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[1;32m   1649\u001B[0m         dl_manager,\n\u001B[1;32m   1650\u001B[0m         verification_mode,\n\u001B[1;32m   1651\u001B[0m         check_duplicate_keys\u001B[38;5;241m=\u001B[39mverification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS\n\u001B[1;32m   1652\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m verification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS,\n\u001B[1;32m   1653\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs,\n\u001B[1;32m   1654\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1000\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[1;32m    996\u001B[0m split_dict\u001B[38;5;241m.\u001B[39madd(split_generator\u001B[38;5;241m.\u001B[39msplit_info)\n\u001B[1;32m    998\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    999\u001B[0m     \u001B[38;5;66;03m# Prepare split will record examples associated to the split\u001B[39;00m\n\u001B[0;32m-> 1000\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split(split_generator, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs)\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1002\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m   1003\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find data file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1004\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1005\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mOriginal error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1006\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)\n\u001B[1;32m   1007\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1486\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split\u001B[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001B[0m\n\u001B[1;32m   1484\u001B[0m job_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pbar:\n\u001B[0;32m-> 1486\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m job_id, done, content \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split_single(\n\u001B[1;32m   1487\u001B[0m         gen_kwargs\u001B[38;5;241m=\u001B[39mgen_kwargs, job_id\u001B[38;5;241m=\u001B[39mjob_id, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_prepare_split_args\n\u001B[1;32m   1488\u001B[0m     ):\n\u001B[1;32m   1489\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   1490\u001B[0m             result \u001B[38;5;241m=\u001B[39m content\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1643\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split_single\u001B[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001B[0m\n\u001B[1;32m   1641\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, SchemaInferenceError) \u001B[38;5;129;01mand\u001B[39;00m e\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1642\u001B[0m         e \u001B[38;5;241m=\u001B[39m e\u001B[38;5;241m.\u001B[39m__context__\n\u001B[0;32m-> 1643\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetGenerationError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while generating the dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1645\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m job_id, \u001B[38;5;28;01mTrue\u001B[39;00m, (total_num_examples, total_num_bytes, writer\u001B[38;5;241m.\u001B[39m_features, num_shards, shard_lengths)\n",
      "\u001B[0;31mDatasetGenerationError\u001B[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T16:48:18.038768Z",
     "start_time": "2025-01-03T16:48:18.027451Z"
    }
   },
   "cell_type": "code",
   "source": "ratings = pd.DataFrame(ratings)",
   "id": "69d35210184b2e0f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ratings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m ratings \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(ratings)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ratings' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "ratings[\"review_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][0])\n",
    "ratings[\"explanation_1\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][0])\n",
    "ratings[\"review_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"rating\"][1])\n",
    "ratings[\"explanation_2\"] = ratings[\"feedback\"].apply(lambda x: x[\"explanation\"][1])\n",
    "ratings = ratings.drop(columns=[\"feedback\"])\n",
    "\n",
    "# Map scores to numeric values\n",
    "conversion_dict = {\"Excellent\": 4, \"Acceptable\": 3, \"Could be Improved\": 2, \"Bad\": 1}\n",
    "ratings[\"score_1\"] = ratings[\"review_1\"].map(conversion_dict)\n",
    "ratings[\"score_2\"] = ratings[\"review_2\"].map(conversion_dict)"
   ],
   "id": "dcf7433c24def6ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T16:34:35.765109Z",
     "start_time": "2025-01-03T16:34:34.728418Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leonbartz/.cache/huggingface/datasets/downloads/3fd9d8b80db4e8ce3de049be3beae23e75ccf024624d7daf007ced34c6f446d4/feedback_test.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/5660 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b48c9ba5f6a4d8f9d3a083c70a5caff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotADirectoryError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1607\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split_single\u001B[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001B[0m\n\u001B[1;32m   1606\u001B[0m _time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m-> 1607\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, record \u001B[38;5;129;01min\u001B[39;00m generator:\n\u001B[1;32m   1608\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_shard_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m writer\u001B[38;5;241m.\u001B[39m_num_bytes \u001B[38;5;241m>\u001B[39m max_shard_size:\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/McGill-NLP--feedbackQA/20c8f938f417c88303bb7041cea9554c1d14667686d7d7c5dda83dd4f39e5dc4/feedbackQA.py:109\u001B[0m, in \u001B[0;36mFeedbackQA._generate_examples\u001B[0;34m(self, filepath)\u001B[0m\n\u001B[1;32m    108\u001B[0m key \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 109\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filepath, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    110\u001B[0m     fbqa \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/streaming.py:75\u001B[0m, in \u001B[0;36mextend_module_for_streaming.<locals>.wrap_auth.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(function)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, download_config\u001B[38;5;241m=\u001B[39mdownload_config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/utils/file_utils.py:943\u001B[0m, in \u001B[0;36mxopen\u001B[0;34m(file, mode, download_config, *args, **kwargs)\u001B[0m\n\u001B[1;32m    942\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblock_size\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 943\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(main_hop, mode, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    944\u001B[0m \u001B[38;5;66;03m# add headers and cookies for authentication on the HF Hub and for Google Drive\u001B[39;00m\n",
      "\u001B[0;31mNotADirectoryError\u001B[0m: [Errno 20] Not a directory: '/Users/leonbartz/.cache/huggingface/datasets/downloads/3fd9d8b80db4e8ce3de049be3beae23e75ccf024624d7daf007ced34c6f446d4/feedback_train.json'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mDatasetGenerationError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m ratings \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMcGill-NLP/feedbackQA\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      2\u001B[0m ratings \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(ratings)\n\u001B[1;32m      4\u001B[0m ratings[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreview_1\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m ratings[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeedback\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrating\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/load.py:2151\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[1;32m   2148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\u001B[38;5;241m.\u001B[39mas_streaming_dataset(split\u001B[38;5;241m=\u001B[39msplit)\n\u001B[1;32m   2150\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[0;32m-> 2151\u001B[0m builder_instance\u001B[38;5;241m.\u001B[39mdownload_and_prepare(\n\u001B[1;32m   2152\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[1;32m   2153\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[1;32m   2154\u001B[0m     verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[1;32m   2155\u001B[0m     num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[1;32m   2156\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m   2157\u001B[0m )\n\u001B[1;32m   2159\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[1;32m   2160\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2161\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[1;32m   2162\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:924\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m    922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    923\u001B[0m     prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[0;32m--> 924\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[1;32m    925\u001B[0m     dl_manager\u001B[38;5;241m=\u001B[39mdl_manager,\n\u001B[1;32m    926\u001B[0m     verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[1;32m    927\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs,\n\u001B[1;32m    928\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs,\n\u001B[1;32m    929\u001B[0m )\n\u001B[1;32m    930\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[1;32m    931\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1648\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001B[0m\n\u001B[1;32m   1647\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_download_and_prepare\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager, verification_mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs):\n\u001B[0;32m-> 1648\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[1;32m   1649\u001B[0m         dl_manager,\n\u001B[1;32m   1650\u001B[0m         verification_mode,\n\u001B[1;32m   1651\u001B[0m         check_duplicate_keys\u001B[38;5;241m=\u001B[39mverification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS\n\u001B[1;32m   1652\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m verification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS,\n\u001B[1;32m   1653\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs,\n\u001B[1;32m   1654\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1000\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[1;32m    996\u001B[0m split_dict\u001B[38;5;241m.\u001B[39madd(split_generator\u001B[38;5;241m.\u001B[39msplit_info)\n\u001B[1;32m    998\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    999\u001B[0m     \u001B[38;5;66;03m# Prepare split will record examples associated to the split\u001B[39;00m\n\u001B[0;32m-> 1000\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split(split_generator, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs)\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1002\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m   1003\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find data file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1004\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1005\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mOriginal error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1006\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)\n\u001B[1;32m   1007\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1486\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split\u001B[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001B[0m\n\u001B[1;32m   1484\u001B[0m job_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pbar:\n\u001B[0;32m-> 1486\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m job_id, done, content \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split_single(\n\u001B[1;32m   1487\u001B[0m         gen_kwargs\u001B[38;5;241m=\u001B[39mgen_kwargs, job_id\u001B[38;5;241m=\u001B[39mjob_id, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_prepare_split_args\n\u001B[1;32m   1488\u001B[0m     ):\n\u001B[1;32m   1489\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   1490\u001B[0m             result \u001B[38;5;241m=\u001B[39m content\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1643\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split_single\u001B[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001B[0m\n\u001B[1;32m   1641\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, SchemaInferenceError) \u001B[38;5;129;01mand\u001B[39;00m e\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1642\u001B[0m         e \u001B[38;5;241m=\u001B[39m e\u001B[38;5;241m.\u001B[39m__context__\n\u001B[0;32m-> 1643\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetGenerationError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while generating the dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1645\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m job_id, \u001B[38;5;28;01mTrue\u001B[39;00m, (total_num_examples, total_num_bytes, writer\u001B[38;5;241m.\u001B[39m_features, num_shards, shard_lengths)\n",
      "\u001B[0;31mDatasetGenerationError\u001B[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "execution_count": 16,
   "source": "",
   "id": "3f9d95d65b534825"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T16:31:41.109769Z",
     "start_time": "2025-01-03T16:31:41.097635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Correlation between 2 human raters:\")\n",
    "print(f\"{ratings['score_1'].corr(ratings['score_2'], method='pearson'):.3f}\")"
   ],
   "id": "6bf43d5d68e07ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between 2 human raters:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ratings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCorrelation between 2 human raters:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mratings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscore_1\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcorr(ratings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscore_2\u001B[39m\u001B[38;5;124m'\u001B[39m],\u001B[38;5;250m \u001B[39mmethod\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpearson\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ratings' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8f35c8824d50aa60"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
